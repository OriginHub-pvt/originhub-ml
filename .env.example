# Weaviate Configuration
WEAVIATE_HOST="localhost"
WEAVIATE_PORT=8081
WEAVIATE_GRPC_PORT=50051
WEAVIATE_MODEL="sentence-transformers/all-MiniLM-L6-v2"
WEAVIATE_COLLECTION="ArticleSummary"

# Model Paths (GGUF format)
# Override these to point to your GGUF model files
MODEL_7B_PATH="models/qwen-7b/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf"
MODEL_1B_PATH="models/qwen-1.5b/model.gguf"

# Model Runtime Options
MODEL_CONTEXT_SIZE=4096
MODEL_N_THREADS=4
MODEL_GPU_LAYERS=30  # Number of layers to offload to GPU (0 for CPU-only)

# Model Preloading
# If true, models load at startup. If false, models load on first use.
MODEL_PRELOAD=true

# API Server Configuration
API_HOST="0.0.0.0"
API_PORT=8000

# Debug Mode
# Set to 1 for verbose logging
AGENTIC_DEBUG=0

# Model Backend
# Options: 'llama_cpp' or 'transformers_gpu'
MODEL_BACKEND=llama_cpp

# GPU Optimization (for CUDA-enabled GPUs)
CUDA_VISIBLE_DEVICES=0
GGML_CUDA_FORCE_CUBLAS=1

# RAG Settings
# Similarity threshold for novelty detection (0.0-1.0)
RAG_NEW_THRESHOLD=0.8

# Transformers GPU Backend Settings (only used when MODEL_BACKEND=transformers_gpu)
GPU_HEAVY_MODEL=Qwen/Qwen2.5-7B-Instruct
GPU_LIGHT_MODEL=Qwen/Qwen2-1.5B-Instruct
GPU_MAX_MEMORY=8.0
GPU_TORCH_DTYPE=float16

OPENAI_API_KEY="openai_api_key_here"